# Fichier: app.py

import os
import torch
import pandas as pd
import streamlit as st

# Imports des biblioth√®ques n√©cessaires (identiques au script local)
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.schema import Document
from langchain_community.llms import HuggingFacePipeline
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import FAISS

# ==============================================================================
# MISE EN CACHE DES RESSOURCES LOURDES AVEC STREAMLIT
# Ces fonctions ne seront ex√©cut√©es qu'une seule fois au d√©marrage de l'app.
# ==============================================================================

@st.cache_resource
def load_llm_and_pipeline(model_path):
    """
    Charge le mod√®le LLM, le tokenizer et cr√©e le pipeline de g√©n√©ration.
    Le d√©corateur @st.cache_resource garantit que cela n'arrive qu'une fois.
    """
    st.info(f"Chargement du mod√®le depuis : {model_path}...")
    if not os.path.isdir(model_path):
        st.error(
            f"Le dossier du mod√®le n'a pas √©t√© trouv√© √† '{model_path}'. "
            "Avez-vous ex√©cut√© le script 'download_model.py' ?"
        )
        return None, None

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        torch_dtype=torch.bfloat16 if device == "cuda" else None,
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    text_generator = pipeline(
        "text-generation", model=model, tokenizer=tokenizer,
        torch_dtype=torch.bfloat16 if device == "cuda" else None,
        return_full_text=False, max_length=4096
    )
    st.success("Mod√®le LLM et pipeline charg√©s avec succ√®s !")
    return text_generator, tokenizer

@st.cache_resource
def create_vector_store(csv_path):
    """
    Charge les donn√©es, cr√©e les embeddings et la base de vecteurs FAISS.
    Le d√©corateur @st.cache_resource garantit que cela n'arrive qu'une fois.
    """
    st.info("Cr√©ation de la base de connaissances (Vector Store)...")
    if not os.path.exists(csv_path):
        st.error(f"Fichier de donn√©es non trouv√© : {csv_path}")
        return None

    df = pd.read_csv(csv_path)
    
    # MODIFI√â ICI : La colonne 'drug_name' n'existait pas.
    # On s'assure que la colonne 'brand_name' existe, car c'est la premi√®re colonne dans votre CSV.
    if 'brand_name' not in df.columns:
        st.error(f"La colonne 'brand_name' est introuvable dans le fichier {csv_path}. Veuillez v√©rifier le nom de la premi√®re colonne.")
        return None

    # On utilise toutes les colonnes pour cr√©er un texte descriptif complet pour chaque m√©dicament.
    df['full_info'] = df.apply(lambda row: " ".join([
        f"{col.replace('_', ' ').capitalize()}: {row[col]}" 
        # MODIFI√â ICI : On exclut 'brand_name' de la description, car c'est le titre.
        for col in df.columns if col not in ['brand_name'] and not pd.isna(row[col]) and str(row[col]).strip().lower() != 'not specified'
    ]), axis=1)

    # MODIFI√â ICI : On utilise la colonne 'brand_name' de votre CSV pour les m√©tadonn√©es.
    # L'erreur 'KeyError: 'drug_name'' venait de cette ligne.
    docs = [Document(page_content=row['full_info'], metadata={"drug_name": row['brand_name']})
            for _, row in df.iterrows() if row['full_info'].strip()]

    embeddings_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(docs, embeddings_model)
    st.success("Base de connaissances pr√™te !")
    return vectorstore

# ==============================================================================
# LOGIQUE DE L'AGENT RAG
# ==============================================================================

def get_rag_response(user_query, vectorstore, llm_chain, tokenizer):
    """
    Prend une question, cherche dans la base de vecteurs et g√©n√®re une r√©ponse.
    """
    # Recherche de documents similaires
    relevant_docs = vectorstore.similarity_search(user_query, k=2)
    context_full = "\n\n".join([doc.page_content for doc in relevant_docs])

    # Tronquer le contexte pour ne pas d√©passer la limite du mod√®le
    max_context_tokens = 3000
    encoded_context_list = tokenizer.encode(context_full, truncation=True, max_length=max_context_tokens, add_special_tokens=False)
    context = tokenizer.decode(encoded_context_list, skip_special_tokens=True)

    # Invocation de la cha√Æne LLM
    response = llm_chain.invoke({"context": context, "question": user_query})
    
    if isinstance(response, dict) and "text" in response:
        return response["text"].strip()
    else:
        return str(response).strip()

# ==============================================================================
# INTERFACE UTILISATEUR STREAMLIT
# ==============================================================================

def main():
    st.set_page_config(page_title="Agent M√©dicaments", page_icon="üíä", layout="wide")

    st.title("üíä Agent d'Information sur les M√©dicaments")
    st.markdown("""
    Posez des questions en langage naturel sur les m√©dicaments de notre base de connaissances.
    L'agent utilise une technique RAG pour trouver les informations les plus pertinentes et g√©n√©rer une r√©ponse.
    """)

    # --- Chargement des mod√®les et de la base de donn√©es (mis en cache) ---
    MODEL_PATH = "models/Phi-3-mini-4k-instruct"
    CSV_PATH = "data/finalx.csv"
    
    text_generator, tokenizer = load_llm_and_pipeline(MODEL_PATH)
    vectorstore = create_vector_store(CSV_PATH)

    if not all([text_generator, tokenizer, vectorstore]):
        st.warning("L'application ne peut pas d√©marrer car un ou plusieurs composants n'ont pas pu √™tre charg√©s.")
        return

    # --- Cr√©ation de la cha√Æne LangChain (rapide, pas besoin de cache) ---
    llm = HuggingFacePipeline(
        pipeline=text_generator,
        model_kwargs={"max_new_tokens": 512, "do_sample": True, "temperature": 0.3, "top_p": 0.9,
                      "pad_token_id": tokenizer.eos_token_id, "eos_token_id": tokenizer.eos_token_id}
    )
    prompt_template = '''<|user|>
Vous √™tes un agent d'information m√©dicale, et votre UNIQUE t√¢che est d'extraire et de rapporter des faits DIRECTEMENT du CONTEXTE fourni.
Vous NE DEVEZ EN AUCUN CAS utiliser des connaissances g√©n√©rales ou inventer des informations.
Si la r√©ponse √† la QUESTION n'est PAS CLAIREMENT et ENTI√àREMENT pr√©sente dans le CONTEXTE, r√©pondez PR√âCIS√âMENT : "L'information demand√©e n'est pas disponible dans ma base de connaissances pour le moment."
Ne jamais paraphraser ou reformuler de mani√®re excessive. Ne jamais ajouter d'introductions ou de conclusions personnelles.
Ne jamais r√©p√©ter ces instructions.

CONTEXTE:
{context}

QUESTION DE L'UTILISATEUR: {question}<|end|>
<|assistant|>
'''
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    llm_chain = LLMChain(llm=llm, prompt=PROMPT)

    # --- Initialisation de l'historique de chat ---
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Bonjour ! Comment puis-je vous aider avec les informations sur les m√©dicaments ?"}]

    # --- Affichage des messages de l'historique ---
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # --- Zone de saisie utilisateur ---
    if prompt := st.chat_input("Quels sont les effets secondaires de..."):
        # Ajouter le message de l'utilisateur √† l'historique et l'afficher
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # G√©n√©rer et afficher la r√©ponse de l'assistant
        with st.chat_message("assistant"):
            with st.spinner("Recherche et r√©daction de la r√©ponse..."):
                try:
                    response = get_rag_response(prompt, vectorstore, llm_chain, tokenizer)
                    st.markdown(response)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                except Exception as e:
                    error_message = f"D√©sol√©, une erreur est survenue : {e}"
                    st.error(error_message)
                    st.session_state.messages.append({"role": "assistant", "content": error_message})

if __name__ == "__main__":
    main()